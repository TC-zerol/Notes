BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding

# BERT：预训练深度双向Transformers用于语言理解

## 摘要

我们介绍一个新的语言表示模型叫做BERT，这代表双向编码表示的Transformers，不像最近的语言表达模型(Peters et al., 2018a; Radford et al., 2018)。BERT被设计用来预训深度双向表示从未标注的文本通过在所有层中联合调整左右上下文。因此，只需一个额外的输出层，就可以对预先训练的BERT模型进行调整，从而为各种任务(如问题回答和语言推理)创建最先进的模型，而无需对特定于任务的体系结构进行大量修改。

BERT概念简单，经验丰富。它在11个自然语言处理任务上获得了新的SOTA结果，包括将GLUE提高到80.5%（7.7%的绝对提升），MultiNLI准确率到达了86.7%（4.6%的绝对提高），SQuAD v1.1问题回答测试F1提高到93.2（1.5个点的绝对提高）和SQuA v2.0 测试F1提高到了83.1（5.1个点的绝对提高）。

## 1 介绍

语言模型预训练已经被证明能够提高很多自然语言处理任务(Dai and Le, 2015; Peters et al.,2018a; Radford et al., 2018; Howard and Ruder,2018)。这个包括句子级别的任务例如自然语言推断(Bowman et al., 2015;Williams et al., 2018)和释义(Dolan and Brockett, 2005)，它的目的是通过整体分析来预测句子之间的关系，以及字符级别的任务，如命名实体识别和问题回答，其中需要模型在字符级别生成细粒度的输出。(Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016)。

目前有两种策略可将预先训练的语言表示应用于下游任务:基于特征和微调。基于特征的方法，如ELMo(Peters
et al., 2018a)，使用特定于任务的体系结构，其中包括作为附加功能的预先训练的表示。微调方法，例如生成预训练Transformer（OpenAI GPT）（Radford et al. 2018）,引入最小的特定于任务的参数，并通过简单地微调所有预训练的参数对下游任务进行训练。这两种方法在训练前共享相同的目标功能，即使用单向语言模型来学习一般的语言表示。

我们认为，当前的技术限制了预先训练的表示的能力，特别是对于微调方法。最主要的限制是标准语言模型是非双向的，这限制了在预训练前使用的架构的选择。例如，在OpenAI GPT,作者使用一个从左到右的结构，每一个字符只能处理在Transformer的自注意力层中之前的字符。（Vaswani et al.,2017）。这种限制对于句子级别的任务来说是次优的，也有可能

