Deep contextualized word representations

# 深度上下文化的单词表示

## 摘要

我们介绍了一种新型的深层语境化单词表示，模型：(1)单词使用的复杂特征(例如，语法和语义)，以及(2)这些用法如何在不同的语言环境中变化(例如，，以模拟一词多义)。我们的词向量是深度双向语言模型(biLM)内部状态的习得函数，该模型是在大型文本语料库上预先训练的。我们表明，这些表示可以很容易地添加到现有的模型中，并通过6个具有挑战性的NLP问题(包括问题回答、文本蕴涵，情感分析)显著地改进现有模型的状态。 我们还提出了一个分析表明，暴露预先训练的网络的深层内部是至关重要的，允许下游模型混合不同类型的半监督信号。 

## 1 介绍

预训练词表示(Mikolov et al.,2013; Pennington et al., 2014)是许多神经语言理解模型的关键组成部分。然而，学习高质量的表现是有挑战性的。理想情况下，它们应该同时模仿这两种模式（1）词汇使用的复杂特征(如语法和语义) ，和(2)这些用法在不同的语境中是如何变化的。即模拟一词多义。在本文中，我们介绍了一种新的深度上下文化单词表示方法，它可以直接解决这两个问题，可以很容易地集成到现有的模型中，并在一系列具有挑战性的语言理解问题中显著地改进每个考虑的情况。

我们的表示不同于传统的词类型嵌入因为每个字符指定了一个表示，该表示是整个输入语句的函数。我们使用从双向LSTM派生的向量，该双向LSTM是在大型文本语料库上经过耦合语言模型（LM）目标训练的。由于这个原因，我们叫他ELMo（嵌入语言模型）表示。不像之前的学习上下文词嵌入方法（Peters et al., 2017; McCann et al., 2017）,ELMo表示是深度的，从某种意义上说，它们是biLM所有内层的函数。更具体地说，我们学习一个线性组合的向量堆叠在每个输入字以上的每一个结束的任务，这显着提高性能比只是使用顶层LSTM层。

以这种方式组合内部状态允许非常丰富的单词表示。使用内在的评价我们证明了较高的层次LSTM状态捕获单词含义的上下文相关方面。(例如，它们可以不加修改地用于监督词意消除歧义任务)，而较低级别的状态则对语法的各个方面进行建模(例如，它们可以用来做词性标注)。同时暴露所有这些信号是非常有益的，允许学习的模型选择对每个最终任务最有用的半监督类型。

大量的实验表明，ELMo表示在实践中效果非常好。我们首先表明，它们可以很容易地添加到现有的六个不同的和具有挑战性的语言理解问题的模型中，包括文本蕴涵、问题回答和情感分析。在每一种情况下，仅仅添加ELMo表现形式就可以显著地提高技术水平，包括高达20%的相对误差的减少。对于能够直接比较的任务，ELMo表现的比CoVe更好(McCann et al., 2017),它计算上下文表示使用一个神经机器翻译编码器。最后，对ELMo和CoVe的分析表明，深层表示优于仅来自LSTM顶层的深层表示。我们的训练模型和代码能够公开获得，我们期望ELMo能够在其他NLP问题上有着相似的提高。

## 2 相关工作

由于它们能够从大量未标记的文本中捕获单词的语法和语义信息，因此可以使用预先训练的单词向量（Turian et al., 2010; Mikolov et al., 2013; Pennington et al.,2014）是其他大部分SOTA语言模型的基本部分。包括问题回到（Liu et al., 2017）,文本蕴涵（Chen et al., 2017）和语言角色标注(He et al.,2017)。然而这些学习单词向量的方法仅允许每个单词有一个独立于上下文的表示。

之前提出的方法克服了一些传统词向量的缺点，通过使用子词信息来丰富它们。(e.g., Wieting et al., 2016; Bojanowski et al., 2017)或者学习每个词义的不同向量(e.g., Neelakantan et al., 2014).我们的方法还受益于通过使用字符卷积的子单词单元，并且我们无缝地将多义信息合并到下游任务中，而不需要显式地培训来预测预定义的义类。

最近其他的工作也关注于学习文本独立的表示context2vec(Melamud et al., 2016)使用双向长短期记忆（LSTM;Hochreiter and Schmidhuber, 2017）来围绕一个中心词对上下文进行编码。其它学习上下文嵌入包括关键字本身在表示和计算与编码器的监督神经机翻译(MT)系统(CoVe;McCann et al., 2017)或非监督语言模式（Peters et al., 2017）.这些方法都受益于大规模的数据集，虽然机器翻译方法受到平行语料库大小的限制。在本文中，我们充分利用丰富的单语数据，对我们的biLM进行约3000万个句子的语料库训练(Chelba et al.， 2014)。我们还将这些方法推广到深层上下文表示，我们发现这些方法在各种NLP任务中都能很好地工作。

先前的工作也展示了不同深度双向RNNs的层编码了不同类型的信息。例如，引入多任务句法监督（即,词性标注）在深度LSTM的较低级别上，可以提高高级任务(如依赖项解析)的整体性能。(Hashimote et al., 2017)或者CCG超标注标签(Søgaard and Goldberg, 2016).Belinkov等人(2017)在基于rnn的编解码器机器翻译系统中发现，在2层LSTM编码器的第一层学习的表示比第二层更能预测POS标签。最后，LSTM的最高层对于编码词上下文(Melamud et al., 2016)已经被证明可以学习词义的表示。我们发现，类似的信号也会被我们的ELMo表示的修改后的语言模型目标所诱导，并且对于混合了这些不同类型的半监督的下游任务来说，学习模型是非常有益的。

Dai and Le (2015) 和 Ramachandran et al.(2017)使用语言模型和序列自动编码器对编解码器对进行预训练，然后在特定任务的监督下进行微调。相反，在使用未标记的数据对biLM进行预培训之后，我们修正了权重并添加了额外的特定于任务的模型容量，允许我们在下游培训数据大小要求较小的监督模型的情况下利用大型、丰富和通用的biLM表示。

## 3 ELMo:语言模型嵌入

不像广泛使用的词嵌入模型(Pennington et al.,2014),ELMo单词表示是整个输入语句的函数，如本节所述。它们是在带有字符卷积的两层biLMs上进行计算的(第3.1节),作为内部网络状态的线性函数(第3.2节)。这种设置允许我们进行半监督学习，其中biLM在大范围内进行了预训练(第3.4节)，并且很容易被合并到现有的广泛的神经NLP体系结构中(第3.3节)。

### 3.1 双向语言模型

给与一个N个字符的序列$(t_1,t_2,...,t_N)$,一个前馈语言模型计算序列的概率通过建模字符$t_k$的概率由给定的历史信息$(t_1,...,t_N)$:
$$
p(t_1,t_2,...,t_N)=\prod_{k=1}^Np(t_k|t_1,t_2,...,t_{k-1})
$$
最近SOTA神经语言模型(J´ozefowicz et al., 2016; Melis et al., 2017; Merity et al., 2017)计算一个内容独立的表示$x_k^{LM}$(经由字符嵌入或者一个CNN字符)