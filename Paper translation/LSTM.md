理解LSTM网络

# 循环神经网络

人类不会每一次都从头开始思考，当你阅读这篇文章时，你会根据你对之前单词的理解来理解每个单词。 你不要扔掉所有东西，然后再从头开始思考。 你的想法有持久性。

传统的神经网络无法做到这一点，这似乎是一个主要的缺点。 例如，假设您想要对电影中每个点发生的事件进行分类。 目前尚不清楚传统神经网络如何利用其对电影中先前事件的推理后续。

循环神经网络解决了这个问题。 它们是带有循环的网络，允许信息持续存在。

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" width=156 height=256 />

递归神经网络具有循环。

在上面框图中，$A$输入$x_t$输出，循环允许信息从网络的一个步骤传递到下一个步骤。

这些循环使得循环神经网络看起来有点神秘。 但是，如果你多想一点，事实证明它们与普通的神经网络并没有什么不同。 可以将循环神经网络视为同一网络的多个副本，每个副本都将消息传递给后继者。 考虑如果我们展开循环会发生什么：

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" width=466 height=156 />

展开的递归神经网络。







这种类似链的性质表明，递归神经网络与序列和列表密切相关。 它们是用于此类数据的神经网络的自然架构。

他们肯定会被使用！ 在过去几年中，将RNN应用于各种问题取得了令人难以置信的成功：语音识别，语言建模，翻译，图像字幕......这个列表还在继续。 我将讨论使用RNNs可以实现的惊人壮举，以及Andrej Karpathy的优秀博客文章，回归神经网络的不合理有效性。 但他们真的很棒。

这些成功的关键在于使用“LSTM”，这是一种非常特殊的递归神经网络，对于许多任务而言，它比标准版本好得多。 几乎所有基于递归神经网络的令人兴奋的结果都是用它们实现的。 这篇论文将探讨这些LSTM。

# 长期依赖问题

RNN的一个吸引力是他们可能能够将先前信息连接到当前任务，例如使用先前的视频帧可能通知对当前帧的理解。 如果RNN可以做到这一点，它们将非常有用。 

有时，我们只需要查看最近的信息来执行当前任务。 例如，考虑一种语言模型，试图根据之前的单词预测下一个单词。 如果我们试图预测“云在天空中”的最后一个词，我们不需要任何进一步的背景 - 很明显下一个词将是天空。 在这种情况下，如果相关信息与所需信息之间的差距很小，则RNN可以学习使用过去的信息。

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" width=456 height=200 />



但也有一些情况需要更多的背景。 考虑尝试预测文本中的最后一个词“我在法国长大......我说流利的法语。”最近的信息表明，下一个词可能是一种语言的名称，但如果我们想缩小哪种语言，我们需要 从更进一步的背景来看，法国的背景。 相关信息与需要变得非常大的点之间的差距是完全可能的。

不幸的是，随着差距的扩大，RNN无法学习连接信息。

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" width=465 height=190 />

理论上，RNN绝对能够处理这种“长期依赖性”。人类可以仔细挑选参数来解决这种形式的玩具问题。 遗憾的是，在实践中，RNN似乎无法学习它们。 Hochreiter（1991）[德国]和Bengio等人对该问题进行了深入探讨。 （1994），他找到了一些非常根本的原因，为什么它可能很难。

值得庆幸的是，LSTM没有这个问题！



长短期内存网络 - 通常只称为“LSTM” - 是一种特殊的RNN，能够学习长期依赖性。 它们是由Hochreiter＆Schmidhuber（1997）介绍的，并且在下面的工作中被许多人精炼和推广.1他们在各种各样的问题上工作得非常好，现在被广泛使用。

LSTM明确旨在避免长期依赖性问题。 长时间记住信息实际上是他们的默认行为，而不是他们难以学习的东西！

所有递归神经网络都具有神经网络重复模块链的形式。 在标准RNN中，该重复模块将具有非常简单的结构，例如单个tanh层。

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" width=465 height=190 />

标准RNN中的重复模块包含单个层。

LSTM也具有这种类似链的结构，但重复模块具有不同的结构。 有四个，而不是一个神经网络层，以一种非常特殊的方式进行交互。

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" width=465 height=190 />

LSTM中的重复模块包含四个交互层。

不要担心发生了什么的细节。 我们将逐步介绍LSTM图。 现在，让我们试着对我们将要使用的符号感到满意。

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" width=465 height=120 />

在上图中，每一行都携带一个整个向量，从一个节点的输出到其他节点的输入。 粉色圆圈表示逐点运算，如矢量加法，而黄色框表示神经网络层。 行合并表示连接，而行分叉表示其内容被复制，副本将转移到不同的位置。

# LSTM背后的核心理念

LSTM的关键是单元状态，水平线贯穿图的顶部。

电池状态有点像传送带。 它直接沿着整个链运行，只有一些次要的线性交互。 信息很容易沿着它不变地流动。

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" width=465 height=190 />

LSTM确实能够移除或添加信息到细胞状态，由称为门的结构精心调节。

Gates是一种可选择通过信息的方式。 它们由S形神经网络层和逐点乘法运算组成。

<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" width=165 height=190 />

sigmoid层输出0到1之间的数字，描述每个组件应该通过多少。 值为零意味着“不让任何东西通过”，而值为1则意味着“让一切都通过！”

LSTM具有三个这样的门，用于保护和控制电池状态。

# LSTM流程

我们的LSTM的第一步是确定我们将从细胞状态中丢弃哪些信息。这个决定是由称为“遗忘门层”的sigmoid层决定的。

