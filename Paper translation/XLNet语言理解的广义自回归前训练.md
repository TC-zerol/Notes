XLNet: Generalized Autoregressive Pretraining for Language Understanding

# XLNet:语言理解的广义自回归前训练

## 摘要

与基于自回归语言建模的训练前处理方法相比，基于自编码的训练前处理方法具有更好的双向上下文建模能力。然而，通过使用蒙板破坏输入，BERT忽略了蒙板位置之间的依赖关系，并遭受了训练前的细微差异。

