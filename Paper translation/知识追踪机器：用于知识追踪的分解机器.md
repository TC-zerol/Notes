# 知识追踪机器：用于知识追踪的分解机器

## 摘要

知识追踪是一个序列预测问题，其目标是预测学生在与学习平台交互过程中通过问题所得到的结果。通过跟踪学生知识的演变，可以优化教学。现有的方法要么基于时间潜在变量模型，要么基于具有时间特征的因子分析。我们这里展示的是因式分解机（FMs）一个回归或分类模型，包括教育文献中的几个现有模型作为特例。特别是加性因子模型，绩效因子模型和多维项目反应理论。我们使用了几个真实的数据集来展示成千上万的用户和项目，FMs可以在学生数据稀疏的情况下准确快速地估计学生的知识，并处理诸如多个知识点和在项目或技能水平上的尝试次数等方面的信息。我们的方法允许适应比现有模型更高维度的学生模型，并提供了一个测试平台来尝试新的特性组合，以改进现有模型。

对学生学习进行建模是能够发现需要进一步关注的学生，或自动推荐相关学习资源的关键。最初，模型是为参加标准化考试的学生开发的，在标准化考试中，学生可以阅读每一个问题陈述，缺少的答案可能被视为不正确。然而在在线平台比如MOOCs上，学生尝试一些训练，但是不看其他的东西。他们也能从不同的尝试中学习。怎样去衡量获取的知识当学生们尝试不同的问题。

我们想要预测一组I学生(比如用户)对一组J个问题(比如项目)的表现(我们可以将问题互换为项目、问题或任务)。每个学生都能尝试一个问题数次。并可能在连续的尝试中学习。我们假设我们观察有序三元组$（i，j，o）\in I \times J \times \{0,1\}$这encode了学生i尝试问题j要么答对$（o=1）$或者答错$（o=0）$。三元组按时间排列。给与一个新$(i',j')$,我们需要去预测学生i‘是否能答对或者答错问题j’。我们还可以假定对用户或项目有额外的了解。

目前为止，各种各样的模型设计用来建模学生，或基于序列预测，或因素分析。现有的大多数技术都使用一维参数对学生或问题进行建模。在本文中，我们将这些模型推广到更高的维度，并成功地训练出了高达20个维度的高效学生模型。我们的这组模型在学生的观察比较稀疏的时候特别方便，比如当一些学生尝试很少的问题，或者一些问题被很少的学生回答的时候，这是在MOOCs等在线平台上经常遇到的数据。

当拟合学生模型时，最好是利用上所以能够获得的资源。为了得到问题的信息我们可以识别出知识的组成部分
(KCs)参与每个问题。这方面的信息通常被编码成q-矩阵的形式，将题目映射到知识元（knowledge components）$q_{jk}$上，如果$q_{jk}=1$代表题目$j$包含知识元$k$,否则$q_{jk}=0$。在这篇文章中，我们还会用$KG（j）$来代表被问题$j$，例如：$KC(j)={k|q_{jk}=1}$。

为了模拟不同的尝试，我们可以记录一个学生尝试一个问题的次数，或者一个学生在与学习材料互动的过程中有多少次机会获得一项技能。

我们的实验特别表明:

- 最好对每个项目(不仅仅是技能)估计一个偏差，而流行的教育数据挖掘(EDM)模型没有这样做。
- EDM中的大多数现有模型不能处理一个项目的多个技能等方面的信息，但是建议的方法可以。
- 侧信息比增加潜在维度更能提高性能。

据我们所知，这是将边信息合并到学生模型中的最通用框架。为了可重复性，我们的实现可以在GitHub上使用。感兴趣的读者可以检查我们的代码并重用它，以便尝试新的组合和设计新的模型。

## 相关工作

在本节中，我们将回顾几种建议的方法来模拟学生的学习。

### 知识追踪

知识追踪的目的是预测学生回答问题的结果顺序。它通常依赖于在整个过程中对学习者的状态进行建模。经过几次尝试，学生们可能最终会达到精通的状态。

最流行的模型是贝叶斯知识跟踪(BKT)，这是一个隐马尔可夫模型(Corbett and Anderson 1994)。然而，BKT无法模拟一个问题可能需要多个KCs的事实。已经提出了新的模型来处理多个子技能，比如特征感知的学生跟踪(FAST)(Gonz´alez-Brenes, Huang, and Brusilovsky 2014).

随着深度学习模型在预测序列方面的成功，深度知识跟踪(DKT)是一种长短时记忆(LSTM)(Piech et al. 2015)它们已被应用于学生建模。几位研究人员已经对这种网络的几种变体进行了复制实验(Xiong et al. 2016; Wilson et al. 2016a;Wilson et al. 2016b)并证明了一些因子分析模型可以与DKT的性能相匹配，我们将会看到。

### 因子分析

因子分析倾向于从数据中学习共同因素，以便归纳观察结果。它们已经成功地应用于矩阵补全，在矩阵补全中，我们假设数据是为(用户、项)对记录的，但是有许多条目丢失了。对于我们的目的来说，序列预测与序列预测的主要区别在于，观察数据的顺序并不重要。但是，如果希望对时间性（temporality）进行编码，则可以使用时间特性(如简单计数器)来补充数据，我们将在稍后看到这一点。接下来$logit=log\frac{p}{1-p}$。

### 项目反映理论

因子分析中最简单的模型不假设几次尝试之间的知识，它是单参数logistic项响应理论模型，也称为Rasch模型:
$$
logit p_{ij}=\theta_i-d_j
$$
其中$\theta_i$对应学生$i$（学生偏置）的能力，$d_j$表示问题j（问题偏置）的难度。我们下文中将项目反映理论称为IRT。最近wilson已经证明IRT效果超过DKT(Wilson et al. 2016b),即使没有时间特征(Gonz´alez-Brenes, Huang, and Brusilovsky 2014)。这可能是因为DKT有很多参数需要估计，所以很容易出现过拟合，而且很难对长序列进行训练。

IRT模型已扩展到多维能力:
$$
\operatorname{logit} p_{i j}=\left\langle\boldsymbol{\theta}_{i}, \boldsymbol{d}_{\boldsymbol{j}}\right\rangle+\delta_{j}
$$
$\delta_k$是项目j（项目偏置）的容易度，多维项目反应理论(MIRT)难以训练(Desmarais and Baker 2012)因此在EDM文献中并不经常遇到，并且心理测量学论文中使用的维度仍然高达4，但我们在本文中展示了如何有效地训练这些模型，最多20个维度

**AFM和PFA** 加性因子模型（AFM）(Cen,Koedinger, and Junker 2006)考虑到学习者对一个项目的尝试次数:
$$
\operatorname{logit} p_{i j}=\sum_{k \in K C(j)} \beta_{k}+\gamma_{k} N_{i k}
$$
$\beta_k$是技巧k的偏置，$\gamma_k$每一个学习技能的机会k的偏置。$N_{i k}$是学生i在一个需要技巧k的问题上尝试的次数。

绩效因素分析(PFA)(Pavlik, Cen, and Koedinger 2009)分别计算积极尝试和消极尝试:
$$
\operatorname{logit} p_{i j}=\sum_{k \in K C(j)} \beta_{k}+\gamma_{k} W_{i k}+\delta_{k} F_{i k}
$$

### 因式分解机

大量的工作创造了学生建模和协同过滤(CF)在推荐系统中的相似性(Bergner et al. 2012; Thai-Nghe et al. 2011).为CF,因子分解机的设计目的是提供一种方法，将关于项目或用户的侧信息编码到模型中。

(Thai-Nghe et al. 2012)和(Sweeney et al. 2016)使用回归形式的因子分解机器进行学生建模(其中使用均方根误差作为度量)，但据我们所知，还没有将其用于学生建模的分类形式。这就是我们在下一节中在本文中要介绍的内容。

## 知识跟踪机器

现在，我们将介绍本文描述的一系列模型——知识追踪机(KTM)。

设N为特征个数。特性可以指学生、练习、知识组件(KCs)、学习机会或关于学习环境的额外信息。例如，有人可能想要模拟这样一个事实，即学生在移动设备或计算机上尝试了一项练习，这可能会影响他们的结果:当使用移动设备时，可能更难键入正确的答案，因此在预测时应该考虑这些数据。

KTMs对观察事件的二进制结果(对或错)的概率建模，基于事件中涉及的所有特性的稀疏权值集。事件中涉及的特征由长度为N的稀疏向量编码$x$,例如$x_i>0$