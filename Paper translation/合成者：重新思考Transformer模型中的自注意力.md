# SYNTHESIZER：重新思考Transformer模型中的自注意力

SYNTHESIZER:Rethinking Self-Attention in Transformer Models

## 摘要

众所周知，点积注意力模型对于SOTA的Transofrmer模型是至关重要的和不可缺失的。但是它真的如此必要吗？这篇文章调查了基于点积的自注意力机制对于Transformer模型的真正重要和贡献。经过大量的实验，我们发现1）随机对齐矩阵（random alignment matrices）的执行结果出人意料的好，2）从token-token（query-key）交互学习的注意力权重并不是那么重要。为了解释这一点，我们提出SYNTHESIZER，这是一个无需 token-token 交互即可学习合成注意力权重的模型。我们实验的结果显示了SYNTHESIZER在MT（EnDe，EnFr），语言模型（LMIB），抽象摘要（CNN/Dailymail），对话生成（PersonaChat）和多任务语言理解（GLUE，SuperGLUE）一系列任务中能完全和vanilla Transformer模型媲美。

## 1.介绍

Transformer模型[Vaswani et al., 2017]在广泛的任务中已得到证明。 这导致近年来Transformer在很大程度上取代了曾经流行的自回归和循环神经网络模型。Transformer模型的核心是键值对点积的注意机制。Transformer模型的成功被广泛地归功于这种自注意力机制，因为完全连接token图能够对长期依赖关系进行建模，从而提供了健壮的归纳偏差。

但是自注意力点积真的如此重要吗？是否有必要通过昂贵的成对点积来学习注意力重量?本文试图进一步了解点积自我注意机制在Transformer模型中的作用。

点积自注意力的基本作用是学习自对齐，即，决定单个token对于序列中其它token的重要性。为此，有一些记忆隐喻和类别支持这种说法。实际上，术语查询、键和值暗示了自我关注模拟了一个基于内容的检索过程，该过程在其核心部分利用了成对的交互。本文对这一过程进行了反思。

与传统观点相反的是，本文假设我们不仅可以不需要点积式的自我注意，还可以完全不需要基于内容的记忆式的自我注意。传统上，注意力权重是在实例或者样本层面上学习的，这里权重由实例级两两成对交互产生。作为结果，由于缺少一致的全局上下文，这些特定于实例的交互经常在不同的实例之间自由地波动。

这篇文章提出SYNTHESIZER，提出了一种新的自对准矩阵综合模型，取代了手动计算成对点积的方法。我们提出了一套多样化的综合功能，并对它们进行了广泛的评估。我们描述了这些合成函数接收到的源信息，即它们是否从单个tokens,token-token相互交互，和或全局任务信息。 直观上来说，合成函数不同的输入能够捕获不同的视图，这可能在结合的时候有用。除了通用标准的Transformer模型，我们展示了有可能获得有竞争力的结果完全来自全局注意力权重而不考虑token-token之间的交互或者任何局部信息。更特别的是，一个随机矩阵SYNTHESIZER模型在WMT 2014 English-German上取得了27.27BLEU的成绩。我们注意到，在某些情况下，可以用更简单的变体取代流行的、公认的基于内容的网络产品注意力，而不需要牺牲很多性能。总的来说，我们相信我们的发现将会促进对自我关注机制的真正作用和效用的进一步研究和讨论。

SYNTHESIZER是完全基于转换的，它只依赖于简单的前馈层，完全不需要点积和显式的token-token交互。需要重申的是，这项工作脱离了查询键值内存存储的隐含概念，并表明随机对齐矩阵在实践中对于许多任务都是足够的。

**我们的贡献** 我们的主要贡献如下：

- 我们提出了合成注意力，一种不需要点积注意力和基于内容的注意力的新的注意力，相反，我们生成独立于token-token依赖关系的对齐矩阵，并探索用于综合注意力矩阵的参数化函数的大杂烩。
- 我们提出了SYNTHESIZER，一种利用综合注意力的新模式。该模型在广泛的语言任务(包括机器翻译和语言建模)方面具有与最先进的转换器模型相媲美的性能。
- 此外，我们证明(1)随机可学习的对齐矩阵执行竞争和(2)token-token依赖是不必要的，在某些任务上使用Transformer模型以实现良好的性能







